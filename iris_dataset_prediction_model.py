# -*- coding: utf-8 -*-
"""IRIS Dataset Prediction Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jFhNZM2YCbPgg3TQw0eTYmOC7c9PtOx4

# ML Project2: IRIS Dataset Prediction Using Logistic Regression

## Import require library
"""

import pandas as pd
import numpy as np

#Importing  Visualization library
import seaborn as sns
import matplotlib.pyplot as plt

#Importing Model from Sklearn
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

#Importing train_test_split from sklearn
from sklearn.model_selection import train_test_split

"""## Step1: Data Collection"""

df=pd.read_csv('/content/drive/My Drive/Iris.csv')
df

print(df.dtypes)
print(df.info())
print(df.shape)
print(df.isnull().sum())
print(df.describe()) #We can check for outliers by looking at the min and max values of each column in relation to the mean.

df_feature=df.iloc[:,[1,2,3,4]]
df_feature

df_target=df.iloc[:,[5]]
df_target

"""## Step2: Data Cleaning & EDA
1. Look at Data Types
2. Check for Missing Values
3. Statistical Overview
4. *Visualizing*

**Visualizing-Correlations**
The Seaborn library has a great heat map visual for mapping the correlations between features. The higher the number is, the greater the correlation between the two elements. A high positive correlation indicates that the two elements have a positive linear relationship (as one increases the other also increases), and a low negative correlation indicates a negative linear relationship (as one increases the other decreases).
"""

sns.heatmap(df_feature.corr(), annot = True);

"""Petal length and width is most correlated with the target, meaning that as these numbers increase, so does the target value. In this case, it means that flowers in class 2 often have longer petal length and width than flowers in class 0. Sepal width is most anti-correlated, indicating that flowers in class 0 have the greatest sepal width than those in class 2. We can also see some intercorrelation between features, for example petal width and length are also highly correlated. This information is not necessarily the best way to analyze the data, but it allows us to start seeing these relationships.

sns.pairplot(data, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, y_vars=None, kind='scatter', diag_kind='auto', markers=None, height=2.5, aspect=1, corner=False, dropna=True, plot_kws=None, diag_kws=None, grid_kws=None, size=None)

Plot pairwise relationships in a dataset.
"""

sns.pairplot(df_feature,size=2,diag_kind='hist')#(data,size=2)#

"""## Step3: Modeling
After cleaned and explored the data,now begin to develop a model. Our goal is to create a Logistic Regression classification model that will predict which class the flower is based on petal and sepal sizes.
"""

#create the model instance
dtree_model = DecisionTreeClassifier()
log_model = LogisticRegression()
svm_model = SVC()

"""## Step4: Training & Test 
Once we separate the features & the target, we can create a train and test class. As the names suggest, we will train our model on the train set, and test the model on the test set. We will randomly select 80% of the data to be in our training, and 20% as test
"""

X_train, X_test, y_train, y_test = train_test_split(df_feature, df_target, test_size=0.2, random_state=1)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""**Standardize**
With the X values split between training and test, now we can standardize the values. This puts the numbers on a consistent scale while keeping the proportional relationship between them.

### Step4b: To train the model use model.fit
"""

#fit the model on the training data
dtree_model.fit(X_train, y_train)
log_model.fit(X_train,y_train)
svm_model.fit(X_train,y_train)

"""## Step5: Evaluate the Model"""

#AttributeError: 'LogisticRegression' object has no attribute 'evaluate'

"""### Step5b: Evaluate the model using Model.score"""

#the score, or accuracy of the model
print(dtree_model.score(X_test, y_test))
print(log_model.score(X_test, y_test))
print(svm_model.score(X_test, y_test))

# Output = 0.9666666666666667

"""### Cross Validation"""

#the test score is already very high, but we can use the cross validated score to ensure the model's strength 
from sklearn.model_selection import cross_val_score
scores = cross_val_score(log_model, X_train, y_train,  cv=10)
print(scores)
print(np.mean(scores))
# Output = 0.966666666666666

"""Without any adjustments or tuning, this model is already performing very well with a test score of .9667 and a cross validation score of .9667. This means that the model is predicting the correct class for the flower about 97% of time. Much higher than the baseline of 33%!"""



"""## Step6: Hyper Parameter Tunning
The amount of regularization to apply during learning can be controlled by a hyperparameter.
A hyperparameter is a parameter of a learning algorithm (not of the
model). As such, it is not affected by the learning algorithm itself; it must be set prior
to training and remains constant during training. If you set the regularization hyperparameter
to a very large value, you will get an almost flat model (a slope close to
zero); the learning algorithm will almost certainly not overfit the training data, but it
will be less likely to find a good solution. Tuning hyperparameters is an important
part of building a Machine Learning system (you will see a detailed example in the
next chapter).

## Step7: Model Predictions
"""

predictions = log_model.predict(X_test)
predictions
print("autual",y_test)

#Save the Model
import pickle
pickle.dump(dtree_model,open('dtree_model.pk','wb'))
pickle.dump(log_model,open('log_model.pk','wb'))
pickle.dump(svm_model,open('svm_model.pk','wb'))

pre=pickle.load(open('svm_model.pk','rb'))
res = pre.score(X_test, y_test)
print(res)

df_new_data= pd.read_excel("/content/drive/My Drive/iris pre.xlsx")
new_data=df_new_data.iloc[:,[1,2,3,4]]
print(new_data.shape)
new_data

predictions_new = log_model.predict(new_data)
predictions_new

prediction_in_frame=pd.DataFrame(predictions_new)
resuls=new_data.join(prediction_in_frame.rename(columns={0:"prediction"}))
resuls

"""## Confusion Matrix
To look more closely at the predictions that the model made, we can use the confusion matrix. In the confusion matrix, the predicted values are the columns and the actual are the rows. It allows us to see where the model makes true and false predictions, and if it predicts incorrectly, we can see which class it is predicting falsely.
"""

from sklearn.metrics import confusion_matrix
pd.DataFrame(confusion_matrix(y_test, predictions))

"""## Classification Report
Another good way to check how your model is performing is by looking at the classification report. It shows the precision, recall, f1 scores, and accuracy scores, and below is a very brief explanation of these features.
Precision: Number of correctly predicted Iris Virginica flowers (10) out of total number of predicted Iris Virginica flowers (10). Precision in predicting Iris Virginica =10/10 = 1.0
Recall: Number of correctly predicted Iris Virginica out of the number of actual Iris Virginica. Recall = 9/10 = .9
F1 Score: This is a harmonic mean of precision and recall. The formula is F1 Score = 2* (precision * recall) / (precision + recall)
Accuracy: Add all the correct predictions together for all classes and divide by the total number of predictions. 29 correct predictions /30 total values = accuracy of .9667.
"""

from sklearn.metrics import classification_report
print(classification_report(y_test, predictions))

"""## Predicted Probabilities
we can look at the probabilities of each row of data being assigned to one of the three classes. By default, the model will assign the item to the class with the highest probability. If we wanted to adjust the accuracy or precision, we could do this by changing the threshold of how high the predicted probability would have to be before it was assigned to that class.
In this case, there is not a consequence to incorrectly assigning a flower to another class, but models used to detect cancer cells adjust their models to ‘assume the worst’ and assign it as a true cancer cell more often. This is used in many cases when it is better to be over cautious than mislabel the cell as safe and healthy.
"""

probs = log_model.predict_proba(X_test)
#put the probabilities into a dataframe for easier viewing
Y_pp = pd.DataFrame(probs, columns=['class_0_pp', 'class_1_pp', 'class_2_pp'])
Y_pp.head(3)